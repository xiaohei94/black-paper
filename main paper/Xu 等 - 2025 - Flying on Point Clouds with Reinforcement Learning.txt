5
2
0
2

r
a

M
1

]

O
R
.
s
c
[

1
v
6
9
4
0
0
.
3
0
5
2
:
v
i
X
r
a

Flying on Point Clouds with Reinforcement Learning

Guangtong Xu†, Tianyue Wu†, Zihan Wang†, Qianhao Wang, and Fei Gao

Fig. 1: Outdoor flight driven by an RL controller with onboard lidar sensing. The top part of the figure includes two different perspectives of the
executed trajectory and point cloud maps generated by the estimation module in the system, Fast-LIO [1]. The left displays the flight trajectory (indicated
by the red curve) from the view of its start point, while the right shows the flight trajectory from the top-down view. The bottom snapshots illustrate the
drone’s states at different progressions along the trajectory.

is

Abstract— A long-cherished vision of drones

to au-
tonomously traverse through clutter to reach every corner of the
world using onboard sensing and computation. In this paper,
we combine onboard 3D lidar sensing and sim-to-real reinforce-
ment learning (RL) to enable autonomous flight in cluttered
environments. Compared to vision sensors, lidars appear to
be more straightforward and accurate for geometric modeling
of surroundings, which is one of the most important cues for
successful obstacle avoidance. On the other hand, sim-to-real
RL approach facilitates the realization of low-latency control,
without the hierarchy of trajectory generation and tracking. We
demonstrate that, with design choices of practical significance,
we can effectively combine the advantages of 3D lidar sensing
and RL to control a quadrotor through a low-level control
interface at 50Hz. The key to successfully learn the policy in
a lightweight way lies in a specialized surrogate of the lidar’s
raw point clouds, which simplifies learning while retaining a
fine-grained perception to detect narrow free space and thin
obstacles. Simulation statistics demonstrate the advantages of
the proposed system over alternatives, such as performing
easier maneuvers and higher success rates at different speed
constraints. With lightweight simulation techniques, the policy
trained in the simulator can control a physical quadrotor,

Guangtong Xu is with the Huzhou Institute, Zhejiang University, Huzhou
313000, China. (e-mail: guangtong xu@163.com). Tianyue Wu, Qianhao
Wang, and Fei Gao are with the College of Control Science and Engineering,
Zhejiang University, Hangzhou 310027, China, and the Huzhou Institute,
Zhejiang University, Huzhou 313000, China. (e-mail: {tianyueh8erobot,
qhwangaa, fgaoaa}@zju.edu.cn). Zihan Wang is with the Department of
Automation, North China Electric Power University (Baoding), Baoding
071003, China, and the Huzhou Institute, Zhejiang University, Huzhou
313000, China. (e-mail: wangzh 0111@163.com)

†Indicates equal contribution.

where the system can dodge thin obstacles and safely traverse
randomly distributed obstacles.

I. INTRODUCTION

The past decade has witnessed the breakthroughs of
autonomous drones from being protected by external lo-
calization devices [2] to flying through clutter outside the
laboratory with only onboard sensing and computing [3].
However, autonomous flight systems can still fail when faced
with complex environments [4], imperfect state estimation
[5], noisy environmental modeling, and high-latency decision
making [6].

With the rapid development of offboard computing and
data generation & utilization techniques, data-driven methods
are expected to solve the above problems. For example,
the expressive power of neural networks allows the burden
of online inference to be reduced through offline training,
thereby reducing control latency [6]. The need for separate
state estimation and environment construction modules can
also be reduced [5], [7], [8]. One of the most user-friendly
ways to obtain robust closed-loop policies is through sim-
to-real interactive learning methods [9], [10] that conduct
training in simulation with massive data generation, such as
sim-to-real reinforcement learning (RL) [11], [12]. At the
cost, these approaches fundamentally suffer from the discrep-
ancies between simulation and the real world, which should
be carefully handled to mitigate. For instance, employing
state estimation [13] or explicit mapping [14] can alleviate








the performance gap during deployment; by downsampling
the high-dimensional exteroception [8], [15], the controller
can generalize well in specific domains and tasks.

Beyond the choice of methods for developing sensorimotor
policy as mentioned above,
the source of environmental
perception is also a key design choice for autonomous
flight. While most existing works use visual sensors, such
as depth cameras, to model the environments, the sensory
data produced by these sensors are typically noisy, which
requires multi-frame fusion, e.g., through occupancy grid
maps [3]. Unfortunately, such approaches can mark grid
cells containing thin obstacles as free regions due to mas-
sive non-obstacle observations around with a probabilistic-
based formulation [16], fundamentally resulting in the loss
of detection of small objects. In contrast, 3D lidars offer
more comprehensive and accurate environmental perception.
With high-resolution direct 3D perception and precise range
measurements, lidars excel at directly detecting small ob-
stacles [4], [17]. Moreover, lidars are becoming smaller and
more affordable, making them increasingly suitable for micro
drone applications [18].

In this paper, we combine 3D lidar sensing with the sim-
to-real RL approach to implement a deployable autonomous
flight system, which can smoothly fly through clutters, even
with small obstacles. A neural controller mapping high-
dimensional exteroception and proprioception to low-level
control commands, e.g., collective thrusts and bodyrates, is
trained using RL. However, the raw point clouds produced
by lidars are enormous in volume, making RL from scratch
challenging. Naive downsampling of point clouds to ease
the burden of learning [8] risks losing detection of small
objects [17] or making it difficult to discern free areas amid
dense obstacles [19]. Therefore, the responses to fine-grained
sensory information without excessive downsampling are
particularly important when faced with dense or fine obsta-
cles. Motivated by this, we design a task-specific representa-
tion for point cloud aggregation as input to the policy, which
is essentially a specialized downsampling of the raw point
clouds and can divide the observed and unknown regions
of surroundings to develop a policy with awareness of the
unknown [14], [17], [20]. Accurate lidar-based local state
estimation, a lightweight yet effective perception simulation
and dynamics domain randomization enable the policy to
drive a micro quadrotor in the real world.

In summary, our main technical contributions are
• design of a task-relevant lidar sensing representation
that enables lightweight training of RL from scratch,
while maintaining a high-resolution environmental per-
ception;

• system integration of onboard lidar sensing and sim-
to-real RL, and its demonstration of driving a physical
quadrotor through clutter with fine obstacles.

II. RELATED WORK

A. Lidar-based Autonomous Flight in Clutter

manifested as the mapping module in the conventional drone
navigation frameworks [3], [20]–[23]. The most common
approach for systems equipped with lidar sensors, similar to
those using depth cameras, employs probabilistic occupancy
grid maps [16], [24], [25] 1. However, this method, which
traditionally conducts massive raycasting operations, incurs
substantial computational costs when applied on lidar sensing
that have extremely large spatial perception ranges. More-
over, this discretized spatial method may designate a grid
cell containing thin obstacles as free when numerous other
rays pass through the cell [17], thereby failing to effectively
preserve lidar perception of thin obstacles. Another approach
is to directly perform motion planning on point cloud maps
[4], [17], [26]–[29], which maximally preserve the original
lidar data information. Compared to grid maps, since point
clouds are inherently unordered and massive, direct extensive
interaction with such maps is extremely time-consuming
for onboard computation. Therefore, most point cloud map-
based approaches extract simplified feasible spaces, also
known as flight corridors [17], [28]–[30], to improve the
efficiency for the motion planning algorithms’ interaction
with environmental perception. However, flight corridor im-
plicitly filters environmental topological information, thereby
losing the solution space. Notably, since point cloud maps
cannot efficiently differentiate between known and unknown
regions like grid maps, most point cloud map-based works
consistently treat unknown areas as safe. To address this
the authors in [17] propose an convex extraction-
issue,
based approach to distinguish known from unknown regions
in point cloud maps using the calibrated FoV of the lidar
sensor, and adopt the Faster strategy [20] for the unknown
region to enable safe navigation. In summary, traditional map
representation methods for autonomous navigation appear
suboptimal for high-precision, wide-range point cloud inputs
from lidar sensors. Our proposed representation for lidar
sensing can perserve the fine-grained sensory data and also
naturally utilize the field-of-view (FoV) of the sensor to
divide the observed and unknown region, without an explicit
extraction of free space.

B. Reinforcement Learning for Autonomous Flight in Clutter

The method presented in [31] is the first efforts that use
a sim-to-real RL method to control a drone to flight in
clutter, where the raw RGB image is input and massive
visual domain randomization is employed for successful sim-
to-real transfer. However, the task scenario only includes
hallways and the control interface is the vehicle’s velocity
rather than low-level control commands. Raw RGB input is
also used in [32] for quadrotor obstacle avoidance, while
the control commands are discretized into simple behav-
iors and the sim-to-real transfer is not demonstrated. The
authors in [33] use a teacher-student framework [15] to
train a depth sensing-based policy to fly at high speed in
a prior known environment. The authors in [8] and [34] use
differentiable simulators to provide first-order gradients for

The problem of integrating perception data with motion
planning has been extensively researched and practiced,

1In the article, we use both the terms of occupancy map or grid map to

refer to methods based on the principles in [16].


the robot and uniformly downsample the point clouds at a
resolution of 0.05m.

We illustrate the construction of the proposed represen-
tation in Fig. 2. Specifically, the k frames of history point
clouds, where the point clouds are sampled at 10Hz, are
transformed into the body frame at the current timestamp t
using the local state estimation, as shown in Fig. 3. Then,
centered on the robot, the space is divided into n partitions
at equal angles within its body frame, as illustrated in Fig.
2(a). n usually has a value of a few thousand. In our
implementation, n is taken to be 3200, which corresponds
to an angular resolution of 4.5deg. Then, the distance (in
meters) from the nearest point cloud, if at least one point is
detected in the partition, to the robot is used as the value of
this partition, where the value over 10m is truncated to 10.
For point cloud-free regions, we compute a dunknown, which is
the distance from the robot to the unknown region computed
by the FoVs of historical k-frames lidar sensing as illustrated
in Fig. 2(b), where 0 < dunknown < 10. We set the value of
this partition as 20 − dunknown. The distinction of unknown
regions inform the policy to perform a partial observability-
aware behavior [14]. The values of the partitions are listed
as an n-dimension input to the policy.

The proprioception part includes the current ego-centric
velocity given by the state estimator, attitude given by the
IMU, the estimated height (z-axis position), and last taken
action. The normalized relative direction in the x-y plane
towards the goal is also input the policy to inform the target.
We discuss the differences and advantages of our special-
ized surrogate of the point clouds over two alternative forms
of point cloud inputs: (i) downsampled raw point clouds and
(ii) occupancy map [14], [16], [38]. A nice wish is to use the
network to process high-dimensional raw sensor inputs and
map them into low-level control commands. However, in our
setup, the number of raw point clouds between a few frames
reaches a magnitude of 105 and we have to downsample
them in a constant dimension for efficient RL training
and alleviating the sim-to-real gap,
typically by uniform
sampling, random sampling, or farthest point sampling (FPS)
[19]. However, when the environment harbors fine obstacles,
it can be difficult for these task-agnostic sampling approaches
to balance the burden of sample volume with the need to
maintain detection of fine objects, and sample the point
clouds into a constant volume. Moreover, the policy cannot
tell the distinction between observed and unknown regions
from the point cloud modality alone, which is demonstrated
crucial for successful obstacle avoidance in some scenarios
[14], [17]. Another common RL input is occupancy maps
transformed from point clouds or depth maps by local state
estimation [16]. This approach, while mitigating the sim-to-
real gap of sensor modeling compared to raw point cloud
inputs and preserving the impact of fine obstacles, struggles
with the trade-off between high dimensionality and accuracy.
The problem of high dimensionality of this uniform space
division approach is exacerbated by the long-range sensing of
lidars. This is also detrimental to RL training, as with limited
computation resource, we can only use a small mini-batch

Fig. 2: The illustration of the task-relevant lidar sensing representation.
(a) The red square cones are examples of partitions for constructing the lidar
sensing input, and the red dots represent the raw point cloud. The grid lines
on the sphere visualize the partitions. (b) On the top is a schematic of the
lidar’s FoV, where the cartoon image is from the Livox Mid-360 website
[18]. On the bottom is the unknown region calculated from the historical
frames of FoV.

sample-efficient policy optimization to control a quadrotor
using acceleration commands in clutter to follow a desired
velocity without explicit position inputs, using depth sensing
they
and monocular optical flow, respectively. However,
aggressively downsample the resolution of the images to
16 × 12 for generalizability, making the policy unaware
of narrow feasible space or small obstacles. The authors in
[35] propose an RL velocity controller for RGB-D-sensing
in dynamic environments with a velocity obstacle safety
shield. Some works [36], [37] use separate representation
learning methods to enable RL from high-dimensional inputs
to decide the acceleration commands for obstacle avoidance.
Recently, there are works [14], [37] proposing the idea to
use RL for speed adaptation for flight in clutter (both of
the mentioned works use a stereo depth sensor) according to
the partially observed environment. The method in [37] ob-
tains speed-adaptive policy by training the policy in varying
complexity environments which incentivizes the behavior of
speed adaptation. Distinguishing this work from those, the
proposed system uses high-resolution 3D lidar sensing and
thrust and bodyrates for control of higher frequency (50Hz).

III. SIM-TO-REAL RL FOR FLYING ON POINT CLOUDS

We wish to control a quadrotor to safely fly through an
environment with obstacles from one point to another with
only lidar sensing and proprioception. In this section, we
first describe how we implement a reinforcement learning
pipeline, including a specialized observation space. We also
detail the simulation setup and the sim-to-real techniques.

A. Learning the Policy

1) Observation Space: The observation space can be
divided into an exteroception part from the lidar sensing and
a proprioception part from an IMU and local state estimator
[1]. We note that, in the real-world deployment, to ease
the computation burden of constructing the exteroception
representation, we first filter the raw point clouds far from


Fig. 3: The system and policy architectures. The system includes algorithmic components such as a MLP neural controller and estimation algorithms.
The lidar sensing representation from MLP encoder, along with velocity v, attitude q, goal direction g, last desired thrust Tlast, and last desired bodyrate
ωlast are fed into the MLP fusion module. The output command is desired thrust T and bodyrate ω.

size for network updates, leading to sub-optimal performance
[39]. Our proposed method, on the other hand, essentially
combines the ideas of downsampling and occupancy maps
by dividing the perceived point clouds according to their
importance: the closer it is perceived, the more danger it
may cause and the higher perception resolution it should be.
The perception of small obstacles is preserved due to a fine-
grained partition, e.g., with a number of a few thousand.
However, due to the fact that there is an information loss
compared to occupancy maps, small obstacles in the distance
may be perceived as larger ones. Empirically, this does not
affect the safety of the system (see Sec. V-B).

2) Action Space: The output of the policy is the desired
thrust and bodyrates at 50Hz. These commands are executed
by an embedded flight controller on the quadrotor. This
interface allows for great maneuverability of the
control
vehicle without introducing a large sim-to-real gap [5], [40],
[41].

3) Termination: The agent only terminates when (i) its z-
axis position falls outside a predefined range, which is set by
the user according to the deployed scenario, and (ii) collision
is detected between the vehicle and the environment, which
is efficient using a pre-built inflated grid map. When the
agent meets the termination condition, the quadrotor is reset
to free space.

4) Reward Function: The reward function can be ex-

pressed as follows:

r = rforward + rthrust + rsmoothness + rmax speed
+rz + rESDF + rcollision + ryaw,

(cid:13) − (cid:13)

(cid:13)pgoal − plast

(cid:13)pgoal − p(cid:13)

where rforward = (cid:13)
(cid:13)
(cid:13), pgoal is
the location of the navigation target, plast is the position of
the quadrotor in current timestamp, and p is the position of
the quadrotor in the last timestamp, which encourages the
quadrotor to fly towards the goal; rthrust = ∥T − g∥, where
T is the magnitude of the collective thrust and g is the magni-
tude of the gravity; rsmoothness = ∥ω∥ + ∥a − alast∥, where

the current

ω = [ωx, ωy, ωz] and a = [ ˆT , ˆωx, ˆωy, ˆωz] are the bodyrate
and the output of the policy at
timestamp,
respectively, and alast is the last taken action, encouraging
a smooth locomotion; rmax speed = −emax{0,∥v∥−vmax} + 1,
where v = [vx, vy, vz] is the velocity of the quadrotor and
vmax is the user-defined velocity constraint; rz = max{z −
zmax, zmin − z, 0}, where z is the z coordination of the
quadrotor in the world frame; rESDF = λ(1−e−kd2
), where
d is the distance from the robot to the nearest point in the
environment, and λ > 0 and k > 0 are some parameters,
which is a shaping reward for the sparse collision penalty
to encourage the quadrotor to move far from the obstacles;
rcollision = −10 applied only when collision is detected;
ryaw = xbody ·v/ ∥v∥, where xbody is the normalized vector
of x-axis of the body frame and · denotes dot product, which
is to encourage the head of the quadrotor to align with its
movement direction. We note that the weight of each reward
component is omitted in the above formulation.

5) Policy Representation: The policy consists of a mul-
to encode the 3200-
tilayer perceptron (MLP) encoder
dimensional exteroceptive input into a 128-dimension hidden
state and an MLP fusion module to fuse the hidden state and
other vectorized inputs. The MLP encoder is implemented
with 3 layers, where the output dimensions of the hidden
states are 128, 64, 64, respectively. The fusion module is
implemented with 4-layers MLPs, where the output dimen-
sions of the hidden states are 128, 256, 256, 128, respectively.
Then a projection layer maps the hidden state into the
4-dimensional action. The neural network architecture is
illustrated in Fig. 3.

B. Simulation

For quadrotor dynamics, we use the air drag augmented
model used in the open-source codes of [42]. We also
simulate the motor delay as a first-order system.

1) Dynamics Domain Randomization: The actuation of
micro drones is noisy and complex aerodynamics effects


B. Training Implementation

The policy is trained with the proximal policy optimization
(PPO) algorithm [44]. We use 1024 environments to collect
data of 300 time steps between two policy updates. Thanks to
the relatively low-dimensionality of the designed representa-
tions for exteroception inputs, we can use a much larger batch
size of 70000 relative to that available when using occupancy
maps (see Sec. V-A) with the same computation device. The
weights of actor and critic networks are not shared. Data
collection is conducted on an i9-14900K CPU and the neural
network optimization is on an NVIDIA RTK 4090 GPU.

V. RESULTS

In this section, we conduct experiments in both simulation

and the real world, and demonstrate that

• the designed representation as input enables lightweight
learning, while using occupancy maps struggles to ef-
fectively learn the policy from scratch (Sec. V-A);
• massive data generation with large mini-batch size (in
our limited computation resource) crucially contribute
to our method (Fig. 4);

• the proposed method demonstrates superior perfor-
mance in simulation when benchmarked with widely
used open-source systems, quantified as success rates
at different maximum speeds (Sec. V-B);

• the success in simulation can be reproduced in the real

world (Sec. V-C).

A. The Proposed Exteroception Representation vs Occu-
pancy Map

1) Setup: In this subsection, we employ the occupancy
map for comparison with our proposed representation. The
occupancy map can also reveal thin obstacles and unknown
areas as described in Sec. III-A. The resolution in each
axis is set as 0.2m. We use 3-layers 3D convolutional
neural networks (CNNs) and 2D CNNs (both with 2 layers
of MLPs) for processing 3D occupancy maps into 128-
dimension features, respectively. The 2D CNN encoder treats
the grid units along the z-axis as the channel dimension.
We run training and evaluation on a fixed environment
distribution for a controllable testbed to track the evolution
of the performance during training. The results are illustrated
in Fig. 4. We employ various parameter settings about the
number of environments for collecting data, denoted as Ne
in Fig. 4, and the mini-batch size, denoted as Nbs. The speed
constraint is set as 3.0m/s and the historical frame number
k is set as 5. The training setting is consistent with that
described in Sec. IV-B.

2) Results: The results show, a bit surprisingly, the naive
application of the PPO algorithm on occupancy maps makes
to establish an effective policy from scratch.
it difficult
We hypothesize that
is caused by the high-
dimensionality of the input and the end-to-end training
scheme. A supervised representation learning method [36],
[37], [45] may be able to alleviate such a problem by

this result

Fig. 4: The evolution of undiscounted return of the proposed exte-
roception representation vs occupancy map during training. Ne and
Nbs denote the number of training environments and mini-batch size,
respectively.

make it almost impossible to simulate the perfect dynamics
transition in a lightweight way. Therefore, we apply spe-
cialized dynamics domain randomization for developing a
robust policy. Before being input to the model, the thrust and
bodyrates are randomized by ±10% and ±8% [40]. We note
that the randomization coefficients around 5% to 10% should
work similarly. Instead of identifying the drag coefficients
accurately, we largely randomize them at ±30%, since we
have the explicit state estimation of velocity to ensure a
reasonable performance of the policy in simulation. We find
that such an approach of brute force helps the exploration of
the RL agent during training.

2) Lidar Sensing Simulation:

Inspired by an effective
lightweight lidar simulator [43], we extract the unique spatio-
temporal patterns of a specific lidar device, by fitting a
parameterized representation of the real-world lidar’s polar
coordinates in the case where we sample the lidar data
at 10Hz, instead of directly applying the simulated point
clouds in the calibrated FoV. Then, in our own simulation
environment, we apply these polar coordinates to directly
index corresponding points on the pre-built obstacle surfaces
represented by point clouds with a resolution of 0.05m,
thereby simulating the lidar sensor data.

IV. SYSTEM AND IMPLEMENTATION

A. System Overview & Setup

The overview of the integrated system is in Fig. 3.
Specifically, a Livox Mid-360 lidar sensor is equipped on
the quadrotor for onboard sensing. The outputs of the policy
are executed by an onboard PX4 Autopilot flight controller2.
The data from the IMU on the flight controller and the point
clouds are fused according to [1] to generate high-frequency
state estimation that
is locally very accurate. The flight
controller is elaborately tuned to respond to the commands
with bodyrates’ delay less than 20ms. We use a Jetson
Orin NX3 as the onboard computer for the computation of
estimation algorithms and GPU inference.

2https://px4.io/
3https://www.nvidia.com/en-us/autonomous-machines/embedded-

systems/jetson-orin/


Fig. 5: Benchmark results with previous systems under different maximum speeds. Figures (a)-(c) illustrate the comparison results of success rate in
scenarios I, II, and III, respectively. Scenario I: A 40m×10m map with around 100 obstacles, and the obstacle radius is randomly sampled within 0.5 ∼
0.7m. Scenario II: The size of the obstacles is set within 0.5 ∼ 0.7m, but the number of obstacles is increased to around 130. Scenario III: The obstacle
radius is randomized between 0.5m ∼ 1.2m, with around 90 obstacles. (d) The example (successful) trajectory results of the compared approaches in
scenario III with the speed constraint of 3.0m/s, and the corresponding flight distances are provided.

decoupling the two issues of RL, i.e., exploration and high-
dimensional observation-to-action mappings, which is be-
yond the scope of this work. Moreover, at a resolution of
0.2m, the dimension of the occupancy map is 50×50×15,
which makes the maximum mini-batch size that can be
used about the magnitude of 1.5 × 104 for 2D CNN and
5 × 103 for 3D CNN in our implementation on a GPU
with a 24G memory. Such a relatively small mini-batch size
can degrade the performance of PPO, as observed in [39].
On the contrary, the employed exteroception representation,
although still at thousands of dimensions, enables a larger
mini-batch size over the magnitude of 1 × 105 and a suc-
cessful implementation of end-to-end RL training without
excessive tuning.

B. Benchmark with Previous Systems in Simulation

1) Setup: We benchmark our system with three repre-
sentative previous obstacle avoidance systems: Fast-Planner
[21], EGO-Planner [22], and EGO-Planner v2 [3]. Fast-
Planner and EGO-Planner are among the most widely used
open-sourced navigation systems in unknown environments.
Fast-Planner applies an unconstrained trajectory optimiza-
tion on an online updated Euclidean Signed Distance Field
(ESDF). EGO-Planner is developed based on a local and
greedy trajectory optimization method directly applied to
the occupancy map without establishing ESDF with an
elaborately designed finite state machine (FSM). The ESDF-
free design of EGO-Planner reduces decision-making latency
compared to alternative systems [21]. Ego-Planner v2 em-
ploys the MINCO trajectory representation [28] instead of
the B-spline used in vanilla EGO-Planner, which improves
the quality and efficiency of trajectory generation.

For a fair comparison, the sensing FoV and frequency
of the benchmarked methods are set the same as those in
this work. The statistics results are illustrated in Fig. 5.
The results are calculated under various speed constraints
with 50 trials. The test scenarios are generated randomly
according to specific parameterized settings: the obstacle
radius in test scenario I is randomly sampled from 0.5m to
0.7m, distributed in a 40m×10m map with the number of
obstacles around 100; the size of obstacles in test scenario
II is consistent with the first scenario, while the number of
obstacles around 130; the obstacle radius in scenario III is
largely randomized among 0.5m to 1.2m, while the number
of obstacles is around 90. The policy is not trained on the
evaluation environments. The k is set as 5 in our experiments.

2) Results:

In low-speed domains, e.g., the maximum
speed is set as 1m/s, all compared methods perform well.
However, when the speed limitations are relaxed, the per-
formance of them degrades drastically, due to their greedy
trajectory optimization design and handcrafted FSMs, espe-
cially for EGO-Planner. The online optimization also causes
a significant perception latency with limited computational
sources [6]. The sub-optimal designs in these systems result
in the occasional sacrifice of dynamical feasibility by the
trajectory optimization methods [14] and the compounding
error of imperfect trajectory tracking control [5], [41], which
also fundamentally contribute to the eventual failure. A
similar phenomenon is observed in [14]. At the same time,
the benchmarked systems lack awareness of the partially
observable nature of the task and rely only on high-frequency
replanning to mitigate the impact caused by unknown en-
vironments. In high-speed settings, such an effect can be


range FoV of the lidar sensing, the quadrotor is able to
demonstrate an easy maneuver, smoothly flying through the
free areas between the box-shaped obstacles.

2) Safe Traversal through Trees: We deploy the system
in an outdoor scenario with unevenly distributed trees, as
shown in Fig. 1. Compared to indoor experiments, outdoor
experiments are challenging not only due to the broad scene,
where parts of the environment frequently exceed the sensing
range resulting in massive invalid measurements, but also
due to the unmodeled wind perturbations. We hypothesize
the policy can handle these unmodeled factors via a large
dynamics domain randomization, as described in III-B. We
train a policy with the maximum speed constraint as 3.0m/s.
In the experimented scenario, the quadrotor flies over 25m
in different trials.

VI. CONCLUSION, LIMITATIONS AND FUTURE WORK

In this work, we present one of the first efforts to combine
3D lidar sensing and sim-to-real RL for autonomous drone
flights. We demonstrate that, without excessive downsam-
pling, we are able to retain the perception of small obstacles
and narrow spaces while achieving lightweight access to
deployable policies without a hierarchy of trajectory genera-
tion and tracking. Thanks to the sound simulation techniques
and accurate lidar-based state estimation, the policy can be
deployed in the real world.

However, we find that the quadrotor cannot always avoid
thin obstacles in the real world. First, this may be because
the reflectivity varies among different materials, which may
result in a loss of detection by the lidar’s rays in some
cases. Algorithmically, we suspect this is attributed to the
idealized simulation methods currently used for lidar sens-
ing, which directly extract point clouds from pre-generated
environments based on the sensor’s pattern. This introduces
an unexpected sim-to-real gap compared to the point clouds
generated by specific lidar sensors (e.g., the Livox Mid-
360) in the real world. Specifically, we observed that even
when facing a wall that theoretically should be perfectly
observable by the lidar, the data obtained from the sensor
contains numerous radius values of 0 in polar coordinates,
indicating many invalid observations. These invalid measure-
ments, however, are not simulated in our current simulation
environment. At the same time, we find that in scenarios
with particularly large-sized obstacles, the algorithm still
tends to greedily acquire the rewards by a fastest-forward
locomotion under the speed constraint and cannot be trained
with a uniform parameter setup to produce a policy that can
succeed in all the simulated scenarios.

In the future, we plan to implement an effective yet
efficient simulation method for lidar sensing, which is still an
open problem [43], and find out an improved training recipe
or policy architecture to support a generalizable policy in the
open world.

REFERENCES

[1] W. Xu and F. Zhang, “Fast-lio: A fast, robust lidar-inertial odometry
package by tightly-coupled iterated kalman filter,” IEEE Robotics and
Automation Letters, vol. 6, no. 2, pp. 3317–3324, 2021.

Fig. 6: Results of indoor flight of the proposed system. (a) Indoor
scenario I. (b) Indoor scenario II. The quadrotor can fly through the cluttered
environments safely. The arrows represent the flight direction. (c) & (d) Two
highlighted trials of thin-obstacle avoidance and corresponding snapshots.

largely amplified under a constant perception frequency.
RL’s formulation alleviates the above problems and mitigates
overly greedy optimization by massively sampling. From
Fig. 5(c), the success rate by ours method maintains around
80%, while other methods are below 40% when the speed
constraint is 4.0m/s. Moreover, from Fig. 5(d), we can see
that our policy executes an easier trajectory and traverses
the environment along a shorter path compared to the EGO-
Planner family.

C. Real-world Demonstrations

In this subsection, we conduct real-world experiments in
both artificial clutters with fine obstacles and outdoor natural
clutters.

1) Safe Traversal through Boxes and Wires: Inspired by
recent impressive model-based lidar navigation works [4],
[17], we design indoor tests with fine obstacles, i.e., wires
with a diameter of 10 mm, where a vision-based commercial
drone fails to avoid these obstacles [17]. Fig. 6 shows
the trajectory rollouts by the policy in the scenario. The
maximum speed constraint of the policy is set to 2.0m/s.

In particular, in both Fig. 6 (c) and (d), we observe an
upward dodge over the wire obstacles. Thanks to the long-


[2] D. Mellinger and V. Kumar, “Minimum snap trajectory generation
and control for quadrotors,” in 2011 IEEE international conference
on robotics and automation.

IEEE, 2011, pp. 2520–2525.

[3] X. Zhou, X. Wen, Z. Wang, Y. Gao, H. Li, Q. Wang, T. Yang, H. Lu,
Y. Cao, C. Xu, et al., “Swarm of micro flying robots in the wild,”
Science Robotics, vol. 7, no. 66, p. eabm5954, 2022.

[4] F. Kong, W. Xu, Y. Cai, and F. Zhang, “Avoiding dynamic small
obstacles with onboard sensing and computation on aerial robots,”
IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7869–7876,
2021.

[5] T. Wu, Y. Chen, T. Chen, G. Zhao, and F. Gao, “Whole-body
control through narrow gaps from pixels to action,” arXiv preprint
arXiv:2409.00895, 2024.

[6] A. Loquercio, E. Kaufmann, R. Ranftl, M. M¨uller, V. Koltun, and
D. Scaramuzza, “Learning high-speed flight in the wild,” Science
Robotics, vol. 6, no. 59, p. eabg5810, 2021.

[7] I. Geles, L. Bauersfeld, A. Romero, J. Xing, and D. Scaramuzza,
“Demonstrating agile flight from pixels without state estimation,”
arXiv preprint arXiv:2406.12505, 2024.

[8] Y. Zhang, Y. Hu, Y. Song, D. Zou, and W. Lin, “Back to newton’s
laws: Learning vision-based agile flight via differentiable physics,”
arXiv preprint arXiv:2407.10648, 2024.

[9] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
and structured prediction to no-regret online learning,” in Proceedings
of the fourteenth international conference on artificial intelligence and
statistics.
JMLR Workshop and Conference Proceedings, 2011, pp.
627–635.

[10] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in
robotics: A survey,” The International Journal of Robotics Research,
vol. 32, no. 11, pp. 1238–1274, 2013.

[11] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. Mc-
Grew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al.,
“Learning dexterous in-hand manipulation,” The International Journal
of Robotics Research, vol. 39, no. 1, pp. 3–20, 2020.

[12] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis,
V. Koltun, and M. Hutter, “Learning agile and dynamic motor skills
for legged robots,” Science Robotics, vol. 4, no. 26, p. eaau5872, 2019.
[13] E. Kaufmann, A. Loquercio, R. Ranftl, M. M¨uller, V. Koltun,
and D. Scaramuzza, “Deep drone acrobatics,” arXiv preprint
arXiv:2006.05768, 2020.

[14] G. Zhao, T. Wu, Y. Chen, and F. Gao, “Learning speed adaptation for
flight in clutter,” IEEE Robotics and Automation Letters, 2024.
[15] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,
“Learning robust perceptive locomotion for quadrupedal robots in the
wild,” Science robotics, vol. 7, no. 62, p. eabk2822, 2022.

[16] H. Moravec and A. Elfes, “High resolution maps from wide an-
gle sonar,” in Proceedings. 1985 IEEE international conference on
robotics and automation, vol. 2.

IEEE, 1985, pp. 116–121.

[17] Y. Ren, F. Zhu, G. Lu, Y. Cai, L. Yin, F. Kong, J. Lin, N. Chen, and
F. Zhang, “Safety-assured high-speed navigation for mavs,” Science
Robotics, vol. 10, no. 98, p. eado6187, 2025.

[18] Livox.

(2025) Livox mid360.

[Online]. Available: https://www.

livoxtech.com/mid-360

[19] Y. Eldar, M. Lindenbaum, M. Porat, and Y. Y. Zeevi, “The farthest
point strategy for progressive image sampling,” IEEE transactions on
image processing, vol. 6, no. 9, pp. 1305–1315, 1997.

[20] J. Tordesillas, B. T. Lopez, M. Everett, and J. P. How, “Faster: Fast
and safe trajectory planner for navigation in unknown environments,”
IEEE Transactions on Robotics, vol. 38, no. 2, pp. 922–938, 2021.

[21] B. Zhou, F. Gao, L. Wang, C. Liu, and S. Shen, “Robust and efficient
quadrotor trajectory generation for fast autonomous flight,” IEEE
Robotics and Automation Letters, vol. 4, no. 4, pp. 3529–3536, 2019.
[22] X. Zhou, Z. Wang, H. Ye, C. Xu, and F. Gao, “Ego-planner: An esdf-
free gradient-based local planner for quadrotors,” IEEE Robotics and
Automation Letters, vol. 6, no. 2, pp. 478–485, 2020.

[23] Z. Han, Z. Wang, N. Pan, Y. Lin, C. Xu, and F. Gao, “Fast-racing: An
open-source strong baseline for se(3) planning in autonomous drone
racing,” IEEE Robotics and Automation Letters, vol. 6, no. 4, pp.
8631–8638, 2021.

[24] Y. Cai, F. Kong, Y. Ren, F. Zhu, J. Lin, and F. Zhang, “Occupancy
grid mapping without ray-casting for high-resolution lidar sensors,”
IEEE Transactions on Robotics, vol. 40, pp. 172–192, 2023.

[25] Y. Ren, Y. Cai, F. Zhu, S. Liang, and F. Zhang, “Rog-map: An efficient
robocentric occupancy grid map for large-scene and high-resolution
lidar-based motion planning,” in 2024 IEEE/RSJ International Con-

ference on Intelligent Robots and Systems (IROS).
8119–8125.

IEEE, 2024, pp.

[26] F. Gao, W. Wu, W. Gao, and S. Shen, “Flying on point clouds: Online
trajectory generation and autonomous navigation for quadrotors in
cluttered environments,” Journal of Field Robotics, vol. 36, no. 4,
pp. 710–733, 2019.

[27] J. Zhang, R. G. Chadha, V. Velivela, and S. Singh, “P-cal: Pre-
computed alternative lanes for aggressive aerial collision avoidance,”
in The 12th International Conference on Field and Service Robotics
(FSR), 2019.

[28] Z. Wang, X. Zhou, C. Xu, and F. Gao, “Geometrically constrained tra-
jectory optimization for multicopters,” IEEE Transactions on Robotics,
vol. 38, no. 5, pp. 3259–3278, 2022.

[29] Y. Ren, F. Zhu, W. Liu, Z. Wang, Y. Lin, F. Gao, and F. Zhang,
“Bubble planner: Planning high-speed smooth quadrotor trajectories
using receding corridors,” in 2022 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS).
IEEE, 2022, pp. 6332–
6339.

[30] Q. Wang, Z. Wang, M. Wang, J. Ji, Z. Han, T. Wu, R. Jin, Y. Gao,
C. Xu, and F. Gao, “Fast iterative region inflation for computing
large 2-d/3-d convex regions of obstacle-free space,” arXiv preprint
arXiv:2403.02977, 2024.

[31] F. Sadeghi and S. Levine, “Cad2rl: Real single-image flight without a

single real image,” arXiv preprint arXiv:1611.04201, 2016.

[32] A. Singla, S. Padakandla, and S. Bhatnagar, “Memory-based deep
reinforcement learning for obstacle avoidance in uav with limited envi-
ronment knowledge,” IEEE transactions on intelligent transportation
systems, vol. 22, no. 1, pp. 107–118, 2019.

[33] Y. Song, K. Shi, R. Penicka, and D. Scaramuzza, “Learning
perception-aware agile flight in cluttered environments,” in 2023 IEEE
International Conference on Robotics and Automation (ICRA).
IEEE,
2023, pp. 1989–1995.

[34] Y. Hu, Y. Zhang, Y. Song, Y. Deng, F. Yu, L. Zhang, W. Lin, D. Zou,
and W. Yu, “Seeing through pixel motion: Learning obstacle avoidance
from optical flow with one camera,” arXiv preprint arXiv:2411.04413,
2024.

[35] Z. Xu, X. Han, H. Shen, H. Jin, and K. Shimada, “Navrl: Learning safe
flight in dynamic environments,” arXiv preprint arXiv:2409.15634,
2024.

[36] M. Kulkarni and K. Alexis, “Reinforcement learning for collision-free
flight exploiting deep collision encoding,” in 2024 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, 2024, pp.
15 781–15 788.

[37] H. Yu, C. De Wagter, and G. C. de Croon, “Mavrl: Learn to
fly in cluttered environments with varying speed,” arXiv preprint
arXiv:2402.08381, 2024.

[38] T. Miki, J. Lee, L. Wellhausen, and M. Hutter, “Learning to
walk in confined spaces using 3d representation,” arXiv preprint
arXiv:2403.00187, 2024.

[39] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dbiak, C. Dennison,
D. Farhi, Q. Fischer, S. Hashme, C. Hesse, et al., “Dota 2 with large
scale deep reinforcement learning,” arXiv preprint arXiv:1912.06680,
2019.

[40] E. Kaufmann, L. Bauersfeld, and D. Scaramuzza, “A benchmark
comparison of learned control policies for agile quadrotor flight,” in
2022 International Conference on Robotics and Automation (ICRA).
IEEE, 2022, pp. 10 504–10 510.

[41] Y. Song, A. Romero, M. M¨uller, V. Koltun, and D. Scaramuzza,
“Reaching the limit in autonomous racing: Optimal control versus
reinforcement learning,” Science Robotics, vol. 8, no. 82, p. eadg1462,
2023.

[42] J. Heeg, Y. Song, and D. Scaramuzza, “Learning quadrotor control
from visual features using differentiable simulation,” arXiv preprint
arXiv:2410.15979, 2024.

[43] F. Kong, X. Liu, B. Tang, J. Lin, Y. Ren, Y. Cai, F. Zhu, N. Chen, and
F. Zhang, “Marsim: A light-weight point-realistic simulator for lidar-
based uavs,” IEEE Robotics and Automation Letters, vol. 8, no. 5, pp.
2954–2961, 2023.

[44] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
preprint

optimization

algorithms,”

policy

arXiv

“Proximal
arXiv:1707.06347, 2017.

[45] A. Stooke, K. Lee, P. Abbeel, and M. Laskin, “Decoupling representa-
tion learning from reinforcement learning,” in International conference
on machine learning. PMLR, 2021, pp. 9870–9879.
